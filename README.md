# DemOpt

## Overview

Convolutional neural networks (CNNs) are very good at recognizing images, but they need to be optimized well to perform well. This work explores how different optimizers perform on the difficult CIFAR-10 picture classification task. We use a variety of techniques, including SGD, Adam, RMSprop, and Adadelta, to train a CNN while closely monitoring important parameters like accuracy, loss, and convergence speed. We shed light on each optimizer's advantages and disadvantages via analysis and visualization, revealing how they affect noisy data, loss landscape navigation, and model generalization. Our results open the door to future improvements in computer vision tasks by offering insightful information on selecting the best optimizer for picture classification problems.


## Contributions

This project invites you to explore the performance of various optimizers on the popular CIFAR-10 image classification task. By training a convolutional neural network (CNN) with different optimization methods and analyzing their results, I hope that you will gain practical understanding of their strengths and weaknesses in addressing image recognition challenges.

**Project Objectives:**

- Implement a simple CNN architecture suitable for CIFAR-10 image classification.
- Train the CNN with several popular optimizers like SGD, Adam, RMSprop, and Adadelta.
- Track key performance metrics such as accuracy, loss, and convergence speed for each optimizer.
- Visualize the training process, comparing learning curves and identifying potential pitfalls.


## Usage

The project prefers to operate using notebooks as opposed to CMD. Consequently, notebooks can be used to restart research.

## Contact me via:

+ Gmail: minh.leduc.0210@gmail.com
+ Linkedin: https://www.linkedin.com/in/minh-le-duc-a62863172/
+ Github: https://github.com/Octo-Opt
